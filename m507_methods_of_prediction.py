# -*- coding: utf-8 -*-
"""M507 Methods of Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKTdFyxZDvGJnQXMLOMpmDOfaavzkom_

# M507 Methods of Prediction

## Introduction

### Problem Statement

### Overview of Business Problem

## Importance of Solving Problem

## Data Collection Strategy

https://www.kaggle.com/datasets/camnugent/california-housing-prices
"""



"""## Data Exploration"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)



data = pd.read_csv("/content/housing.csv")
print("Dataset loaded successfully!")
print(f"Shape: {data.shape}")
data.head()

data.info()
data.describe()

# Check for missing values

missing_values = data.isnull().sum()
missing_percent = (missing_values / len(data)) * 100
missing_df = pd.DataFrame({
    'Missing_Count': missing_values,
    'Percentage': missing_percent
})
print(missing_df[missing_df['Missing_Count'] > 0])
duplicates = data.duplicated().sum()
print(f"Number of duplicate rows: {duplicates}")
print(data['ocean_proximity'].value_counts())
print("\n")
print(data['ocean_proximity'].value_counts(normalize=True) * 100)

# Scatter plot showing geographic distribution with house values
plt.figure(figsize=(12, 8))
scatter = plt.scatter(data['longitude'], data['latitude'],
                     c=data['median_house_value'],
                     cmap='viridis',
                     alpha=0.4,
                     s=10)
plt.colorbar(scatter, label='Median House Value')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Geographic Distribution of California Housing Prices')
plt.grid(True, alpha=0.3)
plt.show()

correlations = data.select_dtypes(include=[np.number]).corr()['median_house_value'].sort_values(ascending=False)
print("Correlation with Median House Value:")
# Correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = data.select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# Scatter plot showing house locations with price
plt.figure(figsize=(12, 8))
scatter = plt.scatter(data['longitude'], data['latitude'],
                     c=data['median_house_value'],
                     cmap='YlOrRd',
                     alpha=0.4,
                     s=data['population']/100)
plt.colorbar(scatter, label='Median House Value')
plt.xlabel('Longitude', fontsize=12)
plt.ylabel('Latitude', fontsize=12)
plt.title('California Housing Prices - Geographic Distribution', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()





"""## Data Preprocessing and Feature Engineering"""

# Create a copy for preprocessing
df = data.copy()
print(f"Working with a copy of the dataset")
print(f"Shape: {df.shape}")
# Check missing values again
print("Missing values before handling:")
print("="*50)
print(df.isnull().sum())
print("\n")

# Strategy: Fill total_bedrooms with median value
print("Strategy: Fill 'total_bedrooms' missing values with median")

# Fill missing values in total_bedrooms with median
median_bedrooms = df['total_bedrooms'].median()
df['total_bedrooms'].fillna(median_bedrooms, inplace=True)

print("Missing values after handling:")
print("="*50)
print(df.isnull().sum())

# Feature Engineering: Create rooms per household
df['rooms_per_household'] = df['total_rooms'] / df['households']
print("Created feature: rooms_per_household")
df[['total_rooms', 'households', 'rooms_per_household']].head()
# Feature Engineering: Create bedrooms per room
df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']
print("Created feature: bedrooms_per_room")
df[['total_bedrooms', 'total_rooms', 'bedrooms_per_room']].head()
# Feature Engineering: Create population per household
df['population_per_household'] = df['population'] / df['households']
print("Created feature: population_per_household")
df[['population', 'households', 'population_per_household']].head()

# Display all new features
print("Dataset with new features:")
print("="*50)
print(f"Shape: {df.shape}")
print("\nNew columns:")
print(df.columns.tolist())



"""## Model Training and Evaluation"""

# Separate features and target variable
X = df.drop('median_house_value', axis=1)
y = df['median_house_value']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
from sklearn.model_selection import train_test_split

# Split data into train and test sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")
print(f"\nTraining set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

# Identify numerical and categorical columns
numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()

print("Numerical Features:")
print(numerical_features)
print("\nCategorical Features:")
print(categorical_features)

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Numerical pipeline: impute missing values and scale
numerical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

print("Numerical pipeline created:")
print(numerical_pipeline)

# Categorical pipeline: impute missing values and one-hot encode
categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

print("Categorical pipeline created:")
print(categorical_pipeline)
# Combine both pipelines
preprocessor = ColumnTransformer([
    ('num', numerical_pipeline, numerical_features),
    ('cat', categorical_pipeline, categorical_features)
])

print("Full preprocessor created:")
print(preprocessor)

# Fit on training data and transform
X_train_processed = preprocessor.fit_transform(X_train)
print(f"Processed training data shape: {X_train_processed.shape}")
print(f"Original training data shape: {X_train.shape}")

# Get feature names after preprocessing
try:
    # Get numerical feature names
    num_feature_names = numerical_features

    # Get categorical feature names after one-hot encoding
    cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)

    # Combine all feature names
    all_feature_names = num_feature_names + list(cat_feature_names)

    print(f"Total features after preprocessing: {len(all_feature_names)}")
    print("\nFeature names:")
    for i, name in enumerate(all_feature_names, 1):
        print(f"{i}. {name}")
except:
    print("Feature names extracted")

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import time

# Split training data into train and validation sets
X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train_processed, y_train, test_size=0.2, random_state=42
)

print(f"Final training set: {X_train_final.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

def build_model1(layers, optimizer, input_dim):
    """
    Build a simple Deep Neural Network
    """
    model = Sequential()

    # First hidden layer
    model.add(Dense(layers[0], activation='relu', input_dim=input_dim))

    # Additional hidden layers if specified
    for units in layers[1:]:
        model.add(Dense(units, activation='relu'))

    # Output layer
    model.add(Dense(1))

    # Compile model
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

    return model

def build_model2(layers, optimizer, input_dim, dropout_rate=0.2):
    """
    Build a Deep Neural Network with Dropout regularization
    """
    model = Sequential()

    # First hidden layer
    model.add(Dense(layers[0], activation='relu', input_dim=input_dim))
    model.add(Dropout(dropout_rate))

    # Additional hidden layers if specified
    for units in layers[1:]:
        model.add(Dense(units, activation='relu'))
        model.add(Dropout(dropout_rate))

    # Output layer
    model.add(Dense(1))

    # Compile model
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

    return model

# Model 1 configurations (Simple DNN) - Optimized
model1_configs = [
    {"layers": [32], "optimizer": Adam(0.01)},
    {"layers": [64], "optimizer": Adam(0.01)},
    {"layers": [32, 16], "optimizer": Adam(0.01)},
    {"layers": [64, 32], "optimizer": Adam(0.01)},
    {"layers": [128], "optimizer": Adam(0.005)},
    {"layers": [128, 64], "optimizer": Adam(0.005)},
    {"layers": [64], "optimizer": RMSprop(0.01)},
    {"layers": [64, 32], "optimizer": RMSprop(0.01)},
    {"layers": [128, 64, 32], "optimizer": Adam(0.01)},
    {"layers": [256], "optimizer": Adam(0.005)},
]

# Model 2 configurations (DNN with Dropout) - Optimized
model2_configs = [
    {"layers": [32], "optimizer": Adam(0.01), "dropout": 0.2},
    {"layers": [64], "optimizer": Adam(0.01), "dropout": 0.2},
    {"layers": [32, 16], "optimizer": Adam(0.01), "dropout": 0.2},
    {"layers": [64, 32], "optimizer": Adam(0.01), "dropout": 0.2},
    {"layers": [128], "optimizer": Adam(0.005), "dropout": 0.3},
    {"layers": [128, 64], "optimizer": Adam(0.005), "dropout": 0.3},
    {"layers": [64], "optimizer": RMSprop(0.01), "dropout": 0.2},
    {"layers": [64, 32], "optimizer": RMSprop(0.01), "dropout": 0.2},
    {"layers": [128, 64, 32], "optimizer": Adam(0.01), "dropout": 0.2},
    {"layers": [256], "optimizer": Adam(0.005), "dropout": 0.25},
]

print(f"Model 1 configurations: {len(model1_configs)}")
print(f"Model 2 configurations: {len(model2_configs)}")

# Store results for Model 1
results_model1 = []
input_dim = X_train_final.shape[1]

print("Training Model 1 (Simple DNN) - All Configurations")
print("="*70)

for idx, config in enumerate(model1_configs, 1):
    print(f"\nConfiguration {idx}/{len(model1_configs)}")
    print(f"Layers: {config['layers']}, Optimizer: {config['optimizer'].__class__.__name__}")

    start_time = time.time()

    # Build model
    model = build_model1(config['layers'], config['optimizer'], input_dim)

    # Train model with early stopping
    history = model.fit(
        X_train_final, y_train_final,
        validation_data=(X_val, y_val),
        epochs=30,  # Reduced from 50
        batch_size=64,  # Increased from 32 for faster training
        verbose=0
    )

    # Predict on validation set
    y_val_pred = model.predict(X_val, verbose=0).flatten()

    # Calculate metrics
    r2 = r2_score(y_val, y_val_pred)
    mse = mean_squared_error(y_val, y_val_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_val, y_val_pred)

    training_time = time.time() - start_time

    # Store results
    results_model1.append({
        'Config': idx,
        'Model': 'Simple DNN',
        'Layers': str(config['layers']),
        'Optimizer': config['optimizer'].__class__.__name__,
        'Learning_Rate': float(config['optimizer'].learning_rate.numpy()),
        'R2_Score': r2,
        'RMSE': rmse,
        'MAE': mae,
        'Training_Time': training_time
    })

    print(f"R² Score: {r2:.4f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}, Time: {training_time:.2f}s")

print("\nModel 1 training completed!")

# Store results for Model 2
results_model2 = []

print("\nTraining Model 2 (DNN with Dropout) - All Configurations")
print("="*70)

for idx, config in enumerate(model2_configs, 1):
    print(f"\nConfiguration {idx}/{len(model2_configs)}")
    print(f"Layers: {config['layers']}, Optimizer: {config['optimizer'].__class__.__name__}, Dropout: {config['dropout']}")

    start_time = time.time()

    # Build model
    model = build_model2(config['layers'], config['optimizer'], input_dim, config['dropout'])

    # Train model with early stopping
    history = model.fit(
        X_train_final, y_train_final,
        validation_data=(X_val, y_val),
        epochs=30,  # Reduced from 50
        batch_size=64,  # Increased from 32 for faster training
        verbose=0
    )

    # Predict on validation set
    y_val_pred = model.predict(X_val, verbose=0).flatten()

    # Calculate metrics
    r2 = r2_score(y_val, y_val_pred)
    mse = mean_squared_error(y_val, y_val_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_val, y_val_pred)

    training_time = time.time() - start_time

    # Store results
    results_model2.append({
        'Config': idx,
        'Model': 'DNN with Dropout',
        'Layers': str(config['layers']),
        'Optimizer': config['optimizer'].__class__.__name__,
        'Learning_Rate': float(config['optimizer'].learning_rate.numpy()),
        'Dropout': config['dropout'],
        'R2_Score': r2,
        'RMSE': rmse,
        'MAE': mae,
        'Training_Time': training_time
    })

    print(f"R² Score: {r2:.4f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}, Time: {training_time:.2f}s")

print("\nModel 2 training completed!")

"""## Model Assessment"""

# Combine results from both models
all_results = results_model1 + results_model2

# Create DataFrame
results_df = pd.DataFrame(all_results)

print(f"Total experiments conducted: {len(results_df)}")
print("\nResults DataFrame shape:", results_df.shape)
results_df.head(10)

# Display all experimental results sorted by R2 Score
results_sorted = results_df.sort_values('R2_Score', ascending=False)

print("EXPERIMENTAL RESULTS - ALL CONFIGURATIONS")
print("="*100)
print(results_sorted.to_string(index=False))

# Summary statistics by model type
print("\nSUMMARY STATISTICS BY MODEL TYPE")
print("="*70)

summary = results_df.groupby('Model').agg({
    'R2_Score': ['mean', 'std', 'min', 'max'],
    'RMSE': ['mean', 'std', 'min', 'max'],
    'MAE': ['mean', 'std', 'min', 'max'],
    'Training_Time': ['mean', 'sum']
}).round(4)

print(summary)

# Display top 5 configurations
print("\nTOP 5 CONFIGURATIONS (Based on R² Score)")
print("="*100)

top_5 = results_sorted.head(5)
for idx, row in top_5.iterrows():
    print(f"\nRank {list(top_5.index).index(idx) + 1}:")
    print(f"  Model: {row['Model']}")
    print(f"  Layers: {row['Layers']}")
    print(f"  Optimizer: {row['Optimizer']} (LR: {row['Learning_Rate']})")
    if 'Dropout' in row and pd.notna(row['Dropout']):
        print(f"  Dropout: {row['Dropout']}")
    print(f"  R² Score: {row['R2_Score']:.4f}")
    print(f"  RMSE: {row['RMSE']:.2f}")
    print(f"  MAE: {row['MAE']:.2f}")
    print(f"  Training Time: {row['Training_Time']:.2f}s")

# Get the best configuration
best_config = results_sorted.iloc[0]

print("BEST MODEL CONFIGURATION")
print("="*70)
print(f"Model Type: {best_config['Model']}")
print(f"Configuration: {best_config['Config']}")
print(f"Layers: {best_config['Layers']}")
print(f"Optimizer: {best_config['Optimizer']}")
print(f"Learning Rate: {best_config['Learning_Rate']}")
if 'Dropout' in best_config and pd.notna(best_config['Dropout']):
    print(f"Dropout Rate: {best_config['Dropout']}")
print(f"\nValidation Performance:")
print(f"  R² Score: {best_config['R2_Score']:.4f}")
print(f"  RMSE: {best_config['RMSE']:.2f}")
print(f"  MAE: {best_config['MAE']:.2f}")
print(f"  Training Time: {best_config['Training_Time']:.2f} seconds")

"""## Final Discussion"""



"""## Conclusion

## Bibliography

Abedi, V., Avula, V., Chaudhary, D., Shahjouei, S., Khan, A., Griessenauer, C.J., Li, J. and Zand, R. (2021). Prediction of long-term stroke recurrence using machine learning models. Journal of Clinical Medicine, 10(6), 1286.


Fernandez-Lozano, C., Gestal, M., Munteanu, C.R., Dorado, J. and Pazos, A. (2021). Random forest-based prediction of stroke outcome. Scientific Reports, 11, 10071.

MacEachern, S.J. and Forkert, N.D. (2021). Machine learning for precision medicine. Genome, 64(4), 416-425.



Sarker, I.H. (2021). Machine learning: Algorithms, real-world applications and research directions. SN Computer Science, 2, 160.
"""

